{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LightGBM (LGBM)\n"
      ],
      "metadata": {
        "id": "dbAPte3C2wfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost와 비슷한데 말 그대로 light 버전이라고 생각하면 된다. 따라서 속도가 빠르고 효율적이고 더 정확하다는 장점을 가지고 있다.\n",
        "- Leaf-wise tree로 split된다.\n",
        "- Overfitting될 수 있는데 tree 높이 (depth) 조절하면서 해결할 수 있다\n",
        "- Hyperparameter 쓸 수 있는 게 많다\n",
        "- 예시로 몇 개 말하자면:\n",
        "\n",
        "\n",
        "objective, metric, is_unbalance, boosting (여기에 우리가 원하는 모델? 같은 거를 넣으면 된다 gbdt, randomforest), num_leaves (leaf node 개수), feature_fraction (feature 사용하는 비율, 0.5이면 feature 반만 사용) (overfitting 문제 해결하는 데 쓰이고 모델 훈련도 더 빠르게 된다), bagging_fraction (feature fraction이랑 비슷한데 feature 대신 observation에 적용), bagging_freq (반복 횟수?), learning_rate, verbose\n"
      ],
      "metadata": {
        "id": "CTgrFYx_2zm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/harsh1kumar/learning/blob/master/machine_learning/santander_trxn_prediction/06_trxn_pred_lightgbm.ipynb  \n",
        "\n",
        "참고할 수 있는 예시코드 (팀 프로젝트는 데이터는 아니지만 원리나 어떻게 코드에 대입할 수 있을지 확인할 때 좋을거 같습니다다"
      ],
      "metadata": {
        "id": "VV9UFYgh9SOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier"
        "아니면"
        "pip install lightgbm"
        
      ],
      "metadata": {
        "id": "O_Pi7WDt6U_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression: 하려면 할 수 있겠지만 3개의 결과 가능성이 있기에 복잡해질 수 있다.\n",
        "KNN: 차원 저주에 민감하기 때문에 사용할 거면 PCA 해야 할 수 있다\n",
        "LDA, QDA\n",
        "Decision Tree\n",
        "SVM\n",
        "LightGBM\n",
        "\n",
        "아니면 automl 돌려서 여러 모델 중 성능 제일 좋은 모델 찾고 parameter tuning 해서 성능 최대한 높이는 방식도 있을 거 같다."
      ],
      "metadata": {
        "id": "QigG7eHs93Uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# autoML"
      ],
      "metadata": {
        "id": "96IcA516-F_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mljar-supervised"
      ],
      "metadata": {
        "id": "YXoVrzqI-H3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from supervised.automl import AutoML"
      ],
      "metadata": {
        "id": "fLQY8TzM-IXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automl = AutoML(algorithms=[\"Decision Tree\", \"Linear\", \"Random Forest\"], ## 이 부분에 원하는 모델 넣으면 될 듯\n",
        "                total_time_limit=5*60)\n",
        "automl.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "M7LdB66z-W2B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
